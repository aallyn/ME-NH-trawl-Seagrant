---
title: Building joint species distribution models to investigate predator-prey relationships
  within coastal Gulf of Maine waters
author: "Andrew Allyn"
date: "9/27/2021"
output: html_document
---

```{r setup, include = FALSE}
library(sf)
library(raster)
library(ncdf4)
library(RCMEMS)
library(VAST)
library(gmRi)
library(tidyverse)
library(lubridate)
library(here)
# Sourcing a few functions from SDM workflow and the TargetsSDM functions
devtools::source_url("https://raw.githubusercontent.com/gulfofmaine/sdm_workflow/main/scratch/aja/TargetsSDM/R/combo_functions.R?token=ABTDHL7LKYMIDVZBGDT2JJTBLL5WK")
devtools::source_url("https://raw.githubusercontent.com/aallyn/NASA-FaCeT/master/tools/enhance_r/enhance_r_funcs.R?token=ABTDHL3RLEMPOEWLOCHRM63BLRW7M")
```

```{r, echo = FALSE}
# Access GMRI CSS Style
gmRi::use_gmri_style_rmd(css_file = "gmri_rmarkdown.css")
```

# Overview

In this project, our third objective is to analyze community and functional group distributions, with a particular focus on joint distributions of key predator-prey species for American lobster and Atlantic cod. To meet this objective, we will use joint species distribution models (JSDMs) to assess changes in species co-occurrence patterns. JSDMs will be fit using the vector auto-regressive spatio-temporal (VAST) modeling structure. Within the modeling structure, we will include a suite of environmental variables (e.g., sea surface and bottom temperature, salinity), along with trying to account for unmeasured spatial and spatio-temporal variability using Gaussian Markov Random Fields. Finally, given our interest on community structure and predator-prey relationships, we will try to include American lobster, Atlantic cod, and a few of their key predator and prey species. Specifically, for American lobster predators, we will have Atlantic cod, spiny dogfish, smooth dogfish, little skate, longhorn sculpin, red hake, haddock, cunner, scup, tautog and black sea bass within a single JSDM. For Atlantic cod prey species, we will include key forage species like herring, sand lance, hakes and other, smaller groundfish. 

# Workflow
Damarius Zurrell has a really nice and simple [diagram](https://damariszurell.github.io/SDM-Intro/) outlining the general species distribution modeling workflow and the key stages of conceptualization, data preparation, model fitting, model assessment and then making predictions using the fitted model in space and time. We will follow a similar process to complete this objective.

## Data preparation
With this model, like other parts of the project, we are especially interested in making use of the Maine-New Hampshire inshore trawl survey data. These data are in the project repository Data folder ("MaineDMR_Trawl_Survey_Catch_Data_2021-05-14.csv"). For the preliminary processing, we will be filtering to species of interest (and sizes?), making sure that we have a full occupancy dataset with imputed absences when species were not caught, etc. From this dataset, we will then grab just unique tow locations (time, lat, lon) and extract environmental characteristics at these locations. In the end, we will have a "tidy" model occupancy dataset that includes species catch information as well as the environmental conditions at tow locations. 

### Biological data
```{r}
# Load in the data
raw_catch_dat<- read.csv(here("Data", "MaineDMR_Trawl_Survey_Catch_Data_2021-05-14.csv"))
raw_tows_dat<- read.csv(here("Data", "MaineDMR_Trawl_Survey_Tow_Data_2021-05-14.csv"))

# Getting a unique tows dataset, some column renaming
tows_dat<- raw_dat %>%
  distinct(DMR_TRIP_IDENTIFIER, latitude, longitude, time) %>%
  rename(., "SeasonYear" = DMR_TRIP_IDENTIFIER, "DECDEG_BEGLAT" = latitude, "DECDEG_BEGLON" = longitude, "Time" = time)

# Unique ID?
tows_dat$ID<- paste(tows_dat$SeasonYear, seq(from = 1, to = nrow(tows_dat)), sep = "_")

# Work with the date column...
tows_dat<- tows_dat %>%
  mutate(., "Date" = as.Date(format(Time), "%Y-%m-%d"))

# Creating absences

# Full sample data set
```

### Environmental data
Although in our original proposal we called out using FVCOM sea surface temperature, bottom temperature and salinity, I think we can make a fairly strong case for wanting to use the European Copernicus Marine Environment Monitoring Service GLORYs product instead. With work on other projects, it seems clear that this is the preferred product of Vince Saba and others at NOAA GFDL. GLORYs is a global ocean reanalysis model, which uses a reduced-order Kalman filter to continually assimilate new observations based on along track altimeter data for sea level anomalies, satelinte sea surface temperatures and sea ice concentrations, and in situ temperature and salinity vertical profile observations. Daily ocean temperature and salinity data are available at a 1/12 degree (~ 8km) horizontal resolution from 1993 to present, with ocean temperatures modeled at 50 different vertical levels.

To access these data, I am going to use some code provided by [Robert Schlegal](https://theoceancode.netlify.app/post/dl_env_data_r/), which leverages [some functions written by Mark Payne](https://github.com/markpayneatwork/RCMEMS). For this all to work, I first had to make sure I had Python and the Motu Client properly installed. Mark's GitHub repo had some nice links for these instructions. 

Another thing worth mentioning here is that this is next section of code is where I think "reproducibility" across labs becomes more complicated. Up until now, all of the code and data we needed has been stored within this GitHub repo and associated R project. Now, though, I am going to be accessing some files that are hosted on Box. While there is a `shared.paths` function in the `gmRi` package to facilitate working through this situation, it will be interesting to see when that breaks and if we need to find more robust solutions, especially as the amount of data increases beyond GitHub's data storage limits. 

```{r}
# Rather than extracting ALL of the GLORYs data, I am going to subset things spatially. We could also further subset things temporally, but in this case, I am interested in all the monthly availavble data from 1993 to present. 

# Getting our spatial subset from an existing Gulf of Maine polygon shapefile
# Path to "RES_Data"
shapefile_root<- gmRi::research_access_paths()$res
# Read in the shapefile
gom_sf<- st_read(paste0(shapefile_root, "Shapefiles/GulfofMainePhysioRegions/PhysioRegions_WGS84.shp"))
# Grab the extent to use later on
extent_gom<- st_bbox(gom_sf)

# Getting our temporal subset information. Again, here I want all month-years, but this could be reduced. 
min_year<- 1993
max_year<- 2019
date_range<- base::expand.grid(min_year:max_year, 1:12) %>% 
  dplyr::rename(year = Var1, month = Var2) %>% 
  arrange(year, month) %>% 
  mutate(year_mon = paste0(year,"-",month)) %>% 
  dplyr::select(year_mon)

# Download function -- this was modified slightly from https://theoceancode.netlify.app/post/dl_env_data_r/
download_GLORYS <- function(date_choice, extent_bbox = extent_gom, out_dir_use = "$HOME/Box/RES_Data/GLORYs/GulfofMaine_Monthly/"){
  
  if(FALSE){
    date_choice<- date_range$year_mon[1]
  }
  
  # The GLORYS script
    # This is a dummy script first generated by using the UI on the CMEMS website
    # No need to change anything here except for the --user and --pwd at the end
    # Please place your CMEMS username and password in those fields
  GLORYS_script <- 'python3 /usr/local/lib/python3.9/site-packages/motuclient.py --motu http://my.cmems-du.eu/motu-web/Motu --service-id GLOBAL_REANALYSIS_PHY_001_030-TDS --product-id global-reanalysis-phy-001-030-monthly --longitude-min -180 --longitude-max 179.9166717529297 --latitude-min -80 --latitude-max 90 --date-min "2018-12-25 12:00:00" --date-max "2018-12-25 12:00:00" --depth-min 0.493 --depth-max 0.4942 --variable thetao --variable bottomT --variable so --variable zos --variable uo --variable vo --variable mlotst --variable siconc --variable sithick --variable usi --variable vsi --out-dir data --out-name test.nc --user aallyn --pwd Maine1985! --config-file $HOME/motuclient_files/config.ini'
  
  # Prep the necessary URL pieces
  date_start <- parse_date(date_choice, format = "%Y-%m")
  # A clever way of finding the end date of any month!
    # I found this on stackoverflow somewhere...
  date_end <- date_start %m+% months(1) - 1
  
  # Set the file name
  file_name <- paste0("GLORYS_", date_choice, ".nc")
  print(file_name)
  
  # Take the chunk of code above and turn it into something useful
  cfg <- parse.CMEMS.script(GLORYS_script, parse.user = T)
  
  # This is where one should make any required changes to the subsetting of the data
  # This is now the magic of the RCMEMS package, which allows us to interface with the Python code as though it were R
  cfg_update <- RCMEMS::update(cfg, variable = "thetao --variable bottomT --variable so --variable zos --variable uo --variable vo --variable mlotst --variable siconc --variable sithick --variable usi --variable vsi",
                               longitude.min = as.character(extent_bbox$xmin),
                               longitude.max = as.character(extent_bbox$xmax),
                               latitude.min = as.character(extent_bbox$ymin),
                               latitude.max = as.character(extent_bbox$ymax),
                               date.min = as.character(date_start),
                               date.max = as.character(date_end),
                               out.dir = out_dir_use,
                               out.name = file_name)
  
  # Download and save the file if needed
  if(file.exists(paste0(out_dir_use, file_name))){
    return()
  } else{
    CMEMS.download(cfg_update)
  }
  Sys.sleep(2) # Give the server a quick breather
}
```

Now, checking to see if we actually need to download and collect the GLORYs data, or, if this has already been done. If we do need to download and collect GLORYs data for a particular variable, this next chunk might take a bit of time to run.
```{r}
# Vector of variables we are interested in getting. Check netcdf file to see other potential available variables.
variables_all<- c("thetao", "bottomT", "so")

# Are these already processed or do we need to run the function?
glorys_extract<- any(!file.exists(c(paste0(gmRi::research_access_paths()$res, "GLORYs/GulfofMaine_Monthly/SST.grd"), paste0(gmRi::research_access_paths()$res, "GLORYs/GulfofMaine_Monthly/BottomTemp.grd"), paste0(gmRi::research_access_paths()$res, "GLORYs/GulfofMaine_Monthly/Salinity.grd"))))

# Which is missing -- update this
variables_all_get<- c("thetao", "bottomT", "so")

if(glorys_extract){
  # Use plyr::l_ply to apply function in parallel
  plyr::l_ply(date_range$year_mon, .fun = download_GLORYS, .parallel = F)

  # Loop through and process...
  all_glorys_files<- list.files(paste0(gmRi::research_access_paths()$res, "GLORYs/GulfofMaine_Monthly/"), full.names = TRUE)
  
  for(h in seq_along(variables_all_get)){
    # Empty raster stack to start
    glorys_stack<- raster::stack()
  crs(glorys_stack)<- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"

    # Loop over variables
    variable_get<- variables_all_get[h]
    variable_name_nice<- switch(variable_get,
                            "so" = "Salinity",
                            "bottomT" = "BottomTemp",
                            "thetao" = "SST")
  
    for(i in seq_along(all_glorys_files)){
      # Loop over time steps
      t<- nc_open(all_glorys_files[[i]])
      lon_nc<- ncvar_get(t, varid = "longitude")
      nlons<- dim(lon_nc)
      lat_nc<- ncvar_get(t, varid = "latitude")
      nlats<- dim(lat_nc)

      # Extract available dates from netCDF file
      times<- ncvar_get(t, var = "time")

      # Make times a little bit easier to handle
      dates_full<- as.Date(as.POSIXct("1950-01-01 00:00:00") + as.difftime(times,units="hours"), format = "%Y-%m-%d")
  
      # Run ncvar_get, adjusting order of start and count as needed
      temp<- ncvar_get(t, varid = variable_get)
  
      # Moving from the array format of temp to a raster stack
      rast_out<- fix_raster(temp, lons.use = lon_nc, lats.use = lat_nc, x.min.use = 1, x.max.use = nlons, y.min.use = 1, y.max.use = nlats)
      names(rast_out)<- dates_full
    
      # Store it
      glorys_stack<- raster::stack(glorys_stack, rast_out)
    }
    
    # Write out compiled raster stack
    crs(glorys_stack)<- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
    writeRaster(glorys_stack, paste0(gmRi::research_access_paths()$res, "GLORYs/GulfofMaine_Monthly/", variable_name_nice, ".grd"), overwrite = TRUE)
  }
}

```

After completing the initial data downloading and processing, we can now extract the environmental variables of interest at a given temporal scale for all of the unique tow locations. To do this, I am going to use a few functions that I have written already that are housed in the FaCeT repository. In particularly, I'll use a `static_extract` function to get depth at each tow location and then a `dynamic_2d_extract` function to get sea surface temperature, bottom temperature and salinity at all tow locations. 
```{r}
# Convert our tows data object, with just unique tow locations, into a sf points object
tows_sf<- st_as_sf(tows_dat, coords = c("DECDEG_BEGLON", "DECDEG_BEGLAT"), crs = st_crs(gom_sf), remove = FALSE)

# Run the static extract function to get depth
depth_rast<- raster(paste0(gmRi::research_access_paths()$res, "/Shapefiles/NEShelf_Etopo1_bathy.tiff"))

tows_sf<- static_extract(depth_rast, cov_name = "Depth", sf_points = tows_sf, date_col_name = "Date", df_sf = "sf")

# Now, get the dynamic covariates files list...
dynamic_covariates_files<- list.files(paste0(gmRi::research_access_paths()$res, "GLORYs/GulfofMaine_Monthly/"), pattern = ".grd", full.names = TRUE)
dynamic_covariates_list_use<- vector("list", length(dynamic_covariates_files))
cov_names<- c("BottomTemp", "Salinity", "SST")
for(i in seq_along(dynamic_covariates_files)){
  dynamic_covariates_list_use[[i]]<- raster::stack(dynamic_covariates_files[[i]])
  names(dynamic_covariates_list_use)[i]<- cov_names[i]
}

# Run the dynamic_2d_extract_wrapper function. How is this going to work given daily observations and MONTHLY data...
tows_out<- dynamic_2d_extract_wrapper(dynamic_covariates_list = dynamic_covariates_list_use, t_summ = "monthly", t_position = NULL, sf_points = tows_sf, date_col_name = "Date", df_sf = "df", out_dir = NULL)
```

With the enhancing completed, can do a quick gut check of seasonal averages through time to see if the general patterns make sense.
```{r}
# Quick gut checks...
var_checks<- tows_out %>%
  group_by(., SeasonYear) %>%
  summarize_at(., c("BottomTemp_monthly", "Salinity_monthly", "SST_monthly"), .funs = c(mean), na.rm = TRUE) %>%
  mutate(., "SeasonYearFactor" = factor(SeasonYear, levels = c("FL00", "SP01", "FL01", "SP02", "FL02", "SP03", "FL03", "SP04", "FL04", "SP05", "FL05", "SP06", "FL06", "SP07", "FL07", "SP08", "FL08", "SP09", "FL09", "SP10", "FL10", "SP11", "FL11", "SP12", "FL12", "SP13", "FL13", "SP14", "FL14", "SP15", "FL15", "SP16", "FL16", "SP17", "FL17")),
         "TimePlot" = as.numeric(SeasonYearFactor))

var_checks_plot_dat<- var_checks %>%
  pivot_longer(., -c(SeasonYear, SeasonYearFactor, TimePlot), "Variable", "Value")
```

```{r, include = TRUE}
var_checks_plot<- ggplot() +
  geom_point(data = var_checks_plot_dat, aes(x = TimePlot, y = value, color = Variable)) +
  geom_line(data = var_checks_plot_dat, aes(x = TimePlot, y = value, color = Variable))
var_checks_plot
```

All looks good there. The final step is join the tow data back to the observational data to get our full "tidy" occurence dataset, ready to fit with a species distribution model or joint species distribution model.